{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n**We are going to work with the well-known supervised machine learning algorithm called k-NN or k-Nearest Neighbors. For this exercise, we will use the Iris data set for classification. The attribute Species of the data set will be the variable that we want to predict.**"},{"metadata":{},"cell_type":"markdown","source":"# Import all the required libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Iris dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/iris/Iris.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop(['Species'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Species']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some Explenatory Data Analysis with Iris"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = df[df.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\ndf[df.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\ndf[df.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph shows relationship between the sepal length and width. Now we will check relationship between the petal length and width."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = df[df.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')\ndf[df.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\ndf[df.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=df)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=df)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=df)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal. "},{"metadata":{},"cell_type":"markdown","source":"# Now let us see how are the length and width are distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\", color_codes=True)\ng = sns.pairplot(df,hue = \"Species\", size=3, markers=[\"o\", \"s\", \"D\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the dataset into training and testing dataset. The testing dataset is generally smaller than training one as it will help in training the model better.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.4,random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model with K nearest neighbour algorithm"},{"metadata":{},"cell_type":"markdown","source":"**To find the best value of K,we apply the loop over range(1,50) and append the accuracy score of each K into the list accu.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\naccu = []\n\nfor i in range(1,50):\n    knn = KNeighborsClassifier(n_neighbors= i)\n    knn.fit(x_train,y_train)\n    pred_i = knn.predict(x_test)\n    w = accuracy_score(y_test,pred_i)\n    accu.append(w)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot the line graph of k = 1 to 50 and accu. to find the K with greatest accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1,50),accu,marker='o',markersize=10,markerfacecolor ='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy is gratest at k=9 so,predict the Species wih n_neighbours = 9**"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors= 9)\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**At k = 9 accuracy is 1.00 so,we cannot take k=9 because of overfitting.\nlet's take k=3**"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors= 3)\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)\n\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate the accuracy of our model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_test,y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 3)) + ' %.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we get the best value of k for our Model i.e. k=3.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}